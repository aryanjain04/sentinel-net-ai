GOOGLE_API_KEY=your_google_api_key_here

# Choose LLM provider:
# - gemini: uses Google Gemini via langchain-google-genai
# - ollama: uses a locally-running Ollama server (http://localhost:11434)
LLM_PROVIDER=gemini

# Gemini model name (examples: gemini-2.0-flash, gemini-1.5-pro)
GEMINI_MODEL=gemini-2.0-flash

# LLM reliability controls
# - LLM_MAX_RETRIES: retries per flow (0 recommended on strict quotas)
# - LLM_STOP_ON_RATE_LIMIT: stop deep analysis immediately when quota is hit
LLM_MAX_RETRIES=0
LLM_STOP_ON_RATE_LIMIT=1

# Only used when LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3

# Paths
PCAP_PATH=path/to/traffic.pcap
CHROMA_PATH=./chroma_db
MODEL_PATH=baseline_rf.joblib