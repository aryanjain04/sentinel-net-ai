GOOGLE_API_KEY=your_google_api_key_here

# Choose LLM provider:
# - gemini: uses Google Gemini via langchain-google-genai
# - ollama: uses a locally-running Ollama server (http://localhost:11434)
LLM_PROVIDER=gemini

# Gemini model name (examples: gemini-2.0-flash, gemini-1.5-pro)
GEMINI_MODEL=gemini-2.0-flash

# Only used when LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3

# Paths
PCAP_PATH=path/to/traffic.pcap
CHROMA_PATH=./chroma_db
MODEL_PATH=baseline_rf.joblib